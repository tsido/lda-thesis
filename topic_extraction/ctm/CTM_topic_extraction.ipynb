{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPHemSab23lLSyBt+OU9kiQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsido/lda-thesis/blob/main/topic_extraction/ctm/CTM_topic_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CTM tuning and topic generation\n",
        "\n",
        "In this notebook Contextual Topic Model is tuned using OCTIS and topics are extracted."
      ],
      "metadata": {
        "id": "EeOBGpQZjDyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!wget --no-check-certificate --output-document=data/enriched_data.csv 'https://raw.githubusercontent.com/tsido/lda-thesis/main/data/enriched_data.csv'\n",
        "!wget --no-check-certificate --output-document=data/optimization_results.csv 'https://raw.githubusercontent.com/tsido/lda-thesis/main/topic_extraction/ctm/optimization_results.csv'\n",
        "\n",
        "!mkdir data/octis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYpYrFaT8BRW",
        "outputId": "089fcec4-e967-4f1d-e2a0-6e2607ac2145"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-15 10:47:52--  https://raw.githubusercontent.com/tsido/lda-thesis/main/data/enriched_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31862266 (30M) [text/plain]\n",
            "Saving to: ‘data/enriched_data.csv’\n",
            "\n",
            "data/enriched_data. 100%[===================>]  30.39M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-12-15 10:47:54 (216 MB/s) - ‘data/enriched_data.csv’ saved [31862266/31862266]\n",
            "\n",
            "--2023-12-15 10:47:55--  https://raw.githubusercontent.com/tsido/lda-thesis/main/topic_extraction/ctm/optimization_results.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2023-12-15 10:47:55 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install octis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eSNKgg-x77lA",
        "outputId": "fbea9749-75f4-4867-931c-3dac65c1655f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting octis\n",
            "  Downloading octis-1.13.1-py2.py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim==4.2.0 (from octis)\n",
            "  Downloading gensim-4.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from octis) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from octis) (1.5.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from octis) (3.6.1)\n",
            "Collecting scikit-learn==1.1.0 (from octis)\n",
            "  Downloading scikit_learn-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-optimize>=0.8.1 (from octis)\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from octis) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from octis) (2.1.0+cu121)\n",
            "Collecting numpy==1.23.0 (from octis)\n",
            "  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting libsvm (from octis)\n",
            "  Downloading libsvm-3.23.0.4.tar.gz (170 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.6/170.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from octis) (2.2.5)\n",
            "Collecting sentence-transformers (from octis)\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from octis) (2.31.0)\n",
            "Collecting tomotopy (from octis)\n",
            "  Downloading tomotopy-0.12.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->octis) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->octis) (6.4.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.0->octis) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.0->octis) (3.2.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize>=0.8.1->octis)\n",
            "  Downloading pyaml-23.9.7-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (8.1.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (2.8.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->octis) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->octis) (4.66.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->octis) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (2023.11.17)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->octis) (4.35.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->octis) (0.16.0+cu121)\n",
            "Collecting sentencepiece (from sentence-transformers->octis)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->octis) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->octis) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->octis) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->octis) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->octis) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->octis) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->octis) (2.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (0.10.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.10.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->octis) (6.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->octis) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->octis) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->octis) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->octis) (0.1.4)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->octis) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->octis) (0.4.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->octis) (1.3.0)\n",
            "Building wheels for collected packages: libsvm, sentence-transformers\n",
            "  Building wheel for libsvm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libsvm: filename=libsvm-3.23.0.4-cp310-cp310-linux_x86_64.whl size=251409 sha256=c70c45379a2085839074680c6149f71f16202e3cab58900d20fa9c3fe52a113c\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/c7/19/a8c85928f8e629654b8e1adb3c8091f0bb77344d0ee9954a85\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=7d3dab1f1f7027290a2b814a181d3b386af36485212d2e533f5225a74c1fcb0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built libsvm sentence-transformers\n",
            "Installing collected packages: sentencepiece, pyaml, numpy, libsvm, tomotopy, scikit-learn, gensim, scikit-optimize, sentence-transformers, octis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.2\n",
            "    Uninstalling gensim-4.3.2:\n",
            "      Successfully uninstalled gensim-4.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "bigframes 0.16.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.2.0 libsvm-3.23.0.4 numpy-1.23.0 octis-1.13.1 pyaml-23.9.7 scikit-learn-1.1.0 scikit-optimize-0.9.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tomotopy-0.12.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T6B1_pchjChx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from octis.models.CTM import CTM\n",
        "from octis.dataset.dataset import Dataset\n",
        "from octis.optimization.optimizer import Optimizer\n",
        "from skopt.space.space import Real, Categorical, Integer\n",
        "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
        "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
        "\n",
        "#from octis.models.contextualized_topic_models.datasets.dataset import CTMDataset\n",
        "from octis.models.contextualized_topic_models.utils.data_preparation import QuickText\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration options\n",
        "\n",
        "# Run or skip optimization step, skip if the optimization data is already available\n",
        "RUN_OPTIMIZER=True\n",
        "\n",
        "num_topics = 46 # Number of topics to generate\n",
        "top_k = 5      # Top words to inspect in metrics\n"
      ],
      "metadata": {
        "id": "AcWwtR2G8QxY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the document data\n",
        "df = pd.read_csv('data/enriched_data.csv')\n",
        "descriptions = df['PreprocessedDescription'].str.split()\n",
        "\n",
        "# Prepare custom dataset in a format described\n",
        "# here: https://github.com/MIND-Lab/OCTIS/tree/master/preprocessed_datasets/sample_dataset\n",
        "\n",
        "# Generate labels for the dataset\n",
        "df['label'] = pd.Categorical(df.apply(lambda x : eval(x['AppStoreGenres'])[-1], axis=1))\n",
        "\n",
        "# we need to split the data to training + testing sets, i.e. include additional columns\n",
        "# in addition to the texts\n",
        "df['split'] = np.random.choice(['train', 'test', 'val'], size=len(df), p=[0.8, 0.1, 0.1])\n",
        "df['split'] = pd.Categorical(df['split'], categories=['train', 'val', 'test'], ordered=True)\n",
        "df = df[['PreprocessedDescription', 'split', 'label']].sort_values(by='split')\n",
        "\n",
        "# TODO Also we likely want to use the non-processed texts for CTM since it wants the\n",
        "# non-processed texts as well\n",
        "\n",
        "df[['PreprocessedDescription', 'split', 'label']].to_csv('data/octis/corpus.tsv', sep='\\t', header=False, index=False)\n",
        "\n",
        "\n",
        "# create the metadata file\n",
        "# TODO fix the indexes to be correct!\n",
        "metadata = { 'total_documents': len(df),\n",
        "            'vocabulary_length': 2000,\n",
        "            'preprocessing-info': [],\n",
        "             'labels': [],\n",
        "             'total_labels': 0,\n",
        "             'last-training-doc': 10000,\n",
        "             'last-validation-doc': 11000\n",
        "             }\n",
        "with open('data/octis/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "# Create the vocabulary.txt file using Gensim and keep 2000 most relevant words only\n",
        "dictionary = Dictionary(descriptions)\n",
        "\n",
        "# remove tokens that don't occur in at least 3 documents\n",
        "# and occur in over 50% of docs, keep 2000 words for vocabulary\n",
        "dictionary.filter_extremes(no_below=3, no_above=0.5, keep_n=2000)\n",
        "\n",
        "# create vocabulary.txt file by getting unique words from the dictionary\n",
        "with open(\"data/octis/vocabulary.txt\", \"w\") as f:\n",
        "    for (word) in dictionary.itervalues():\n",
        "      f.write(word +'\\n')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "14L1poUo8XZ_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For CTM we also require the BERT embeddings\n",
        "\n",
        "# TODO\n",
        "#ctm_dataset = QuickText()\n",
        "\n",
        "#octis_dataset.CTMDataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "NrEDACdE2yL7",
        "outputId": "76051bbf-ef25-4b8a-9219-dc7791c83003"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-35ba465125f6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mctm_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuickText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moctis_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCTMDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: QuickText.__init__() missing 2 required positional arguments: 'bert_model' and 'text_for_bow'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "octis_dataset = Dataset()\n",
        "octis_dataset.load_custom_dataset_from_folder('data/octis');\n"
      ],
      "metadata": {
        "id": "gc7j1bDl8hAE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO there is a issue here in the sense that OCTIS\n",
        "# only works for the zeroshot version which doesn't use the\n",
        "# BoW representation at all.\n",
        "\n",
        "# See if that works for us\n",
        "\n",
        "# we're interested in english only, so using roberta as the base contextual model\n",
        "# for SBERT underneath that's underneath the CTM\n",
        "# See https://colab.research.google.com/github/MIND-Lab/OCTIS/blob/master/examples/OCTIS_Optimizing_CTM.ipynb#scrollTo=i6Sywe4vCJW5 for\n",
        "# guidance\n",
        "# NOTE: this only works for zeroshot inference, not the Combined that we thought we\n",
        "# would use...\n",
        "model = CTM(num_topics=num_topics, num_epochs=30, inference_type='zeroshot', bert_model=\"paraphrase-distilroberta-base-v2\")\n",
        "\n",
        "# Evaluation metric\n",
        "npmi = Coherence(texts=octis_dataset.get_corpus())\n",
        "diversity = TopicDiversity(topk=top_k)\n",
        "\n",
        "search_space = {\"num_layers\": Categorical({1, 2, 3}),\n",
        "                \"num_neurons\": Categorical({100, 300, 500}),\n",
        "                \"activation\": Categorical({'rrelu', 'relu'}),\n",
        "                \"dropout\": Real(0.0, 0.95)\n",
        "}\n",
        "\n",
        "optimization_runs=200\n",
        "model_runs=1\n",
        "\n",
        "optimizer=Optimizer()\n",
        "optimization_result = optimizer.optimize(\n",
        "    model, octis_dataset, npmi, search_space, number_of_call=optimization_runs,\n",
        "    model_runs=model_runs, save_models=True,\n",
        "    extra_metrics=[diversity], # to keep track of other metrics\n",
        "    save_path='data/',\n",
        "    early_stop=True,\n",
        "    early_step=5\n",
        "    )\n",
        "\n",
        "\n",
        "optimization_result.save_to_csv(\"optimization_results.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s5QgUwS98qe",
        "outputId": "35c0be54-c648-4a7d-d15a-c29761303e10"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current call:  0\n",
            "Current call:  1\n",
            "Current call:  2\n",
            "Current call:  3\n",
            "Current call:  4\n",
            "Current call:  5\n",
            "Current call:  6\n",
            "Current call:  7\n",
            "Current call:  8\n",
            "Current call:  9\n",
            "Current call:  10\n",
            "Current call:  11\n",
            "Current call:  12\n",
            "Current call:  13\n",
            "Current call:  14\n",
            "Current call:  15\n",
            "Current call:  16\n",
            "Current call:  17\n",
            "Current call:  18\n",
            "Current call:  19\n",
            "Current call:  20\n",
            "Current call:  21\n",
            "Current call:  22\n",
            "Current call:  23\n",
            "Current call:  24\n",
            "Current call:  25\n",
            "Current call:  26\n",
            "Current call:  27\n",
            "Current call:  28\n",
            "Current call:  29\n",
            "Current call:  30\n",
            "Current call:  31\n",
            "Current call:  32\n",
            "Current call:  33\n",
            "Current call:  34\n",
            "Current call:  35\n",
            "Current call:  36\n",
            "Current call:  37\n",
            "Current call:  38\n",
            "Current call:  39\n",
            "Current call:  40\n",
            "Current call:  41\n",
            "Current call:  42\n",
            "Current call:  43\n",
            "Current call:  44\n",
            "Current call:  45\n",
            "Current call:  46\n",
            "Current call:  47\n",
            "Current call:  48\n",
            "Current call:  49\n",
            "Current call:  50\n",
            "Current call:  51\n",
            "Current call:  52\n",
            "Current call:  53\n",
            "Current call:  54\n",
            "Current call:  55\n",
            "Current call:  56\n",
            "Current call:  57\n",
            "Current call:  58\n",
            "Current call:  59\n",
            "Current call:  60\n",
            "Current call:  61\n",
            "Current call:  62\n",
            "Current call:  63\n",
            "Current call:  64\n",
            "Current call:  65\n",
            "Current call:  66\n",
            "Current call:  67\n",
            "Current call:  68\n",
            "Current call:  69\n",
            "Current call:  70\n",
            "Current call:  71\n",
            "Current call:  72\n",
            "Current call:  73\n",
            "Current call:  74\n",
            "Current call:  75\n",
            "Current call:  76\n",
            "Current call:  77\n",
            "Current call:  78\n",
            "Current call:  79\n",
            "Current call:  80\n",
            "Current call:  81\n",
            "Current call:  82\n",
            "Current call:  83\n",
            "Current call:  84\n",
            "Current call:  85\n",
            "Current call:  86\n",
            "Current call:  87\n",
            "Current call:  88\n",
            "Current call:  89\n",
            "Current call:  90\n",
            "Current call:  91\n",
            "Current call:  92\n",
            "Current call:  93\n",
            "Current call:  94\n",
            "Current call:  95\n",
            "Current call:  96\n",
            "Current call:  97\n",
            "Current call:  98\n",
            "Current call:  99\n",
            "Current call:  100\n",
            "Current call:  101\n",
            "Current call:  102\n",
            "Current call:  103\n",
            "Current call:  104\n",
            "Current call:  105\n",
            "Current call:  106\n",
            "Current call:  107\n",
            "Current call:  108\n",
            "Current call:  109\n",
            "Current call:  110\n",
            "Current call:  111\n",
            "Current call:  112\n",
            "Current call:  113\n",
            "Current call:  114\n",
            "Current call:  115\n",
            "Current call:  116\n",
            "Current call:  117\n",
            "Current call:  118\n",
            "Current call:  119\n",
            "Current call:  120\n",
            "Current call:  121\n",
            "Current call:  122\n",
            "Current call:  123\n",
            "Current call:  124\n",
            "Current call:  125\n",
            "Current call:  126\n",
            "Current call:  127\n",
            "Current call:  128\n",
            "Current call:  129\n",
            "Current call:  130\n",
            "Current call:  131\n",
            "Current call:  132\n",
            "Current call:  133\n",
            "Current call:  134\n",
            "Current call:  135\n",
            "Current call:  136\n",
            "Current call:  137\n",
            "Current call:  138\n",
            "Current call:  139\n",
            "Current call:  140\n",
            "Current call:  141\n",
            "Current call:  142\n",
            "Current call:  143\n",
            "Current call:  144\n",
            "Current call:  145\n",
            "Current call:  146\n",
            "Current call:  147\n",
            "Current call:  148\n",
            "Current call:  149\n",
            "Current call:  150\n",
            "Current call:  151\n",
            "Current call:  152\n",
            "Current call:  153\n",
            "Current call:  154\n",
            "Current call:  155\n",
            "Current call:  156\n",
            "Current call:  157\n",
            "Current call:  158\n",
            "Current call:  159\n",
            "Current call:  160\n",
            "Current call:  161\n",
            "Current call:  162\n",
            "Current call:  163\n",
            "Current call:  164\n",
            "Current call:  165\n",
            "Current call:  166\n",
            "Current call:  167\n",
            "Current call:  168\n",
            "Current call:  169\n",
            "Current call:  170\n",
            "Current call:  171\n",
            "Current call:  172\n",
            "Current call:  173\n",
            "Current call:  174\n",
            "Current call:  175\n",
            "Current call:  176\n",
            "Current call:  177\n",
            "Current call:  178\n",
            "Current call:  179\n",
            "Current call:  180\n",
            "Current call:  181\n",
            "Current call:  182\n",
            "Current call:  183\n",
            "Current call:  184\n",
            "Current call:  185\n",
            "Current call:  186\n",
            "Current call:  187\n",
            "Current call:  188\n",
            "Current call:  189\n",
            "Current call:  190\n",
            "Current call:  191\n",
            "Current call:  192\n",
            "Current call:  193\n",
            "Current call:  194\n",
            "Current call:  195\n",
            "Current call:  196\n",
            "Current call:  197\n",
            "Current call:  198\n",
            "Current call:  199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimization_result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9ZGNjJG-HFxn",
        "outputId": "2aca85e0-cebb-4155-a471-90c2613d6b10"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'c_npmi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.read_csv('optimization_results.csv')\n",
        "\n",
        "results.loc[results['Mean(model_runs)'].idxmax()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2x7p4GF2MqJ",
        "outputId": "aafe82cf-99d6-44fb-eb57-3bae2c7ae778"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dataset                           dataset_name\n",
              "surrogate model                             RF\n",
              "acquisition function                       LCB\n",
              "num_iteration                              121\n",
              "time                                 50.845753\n",
              "Median(model_runs)                    0.119142\n",
              "Mean(model_runs)                      0.119142\n",
              "Standard_Deviation(model_runs)             0.0\n",
              "activation                               rrelu\n",
              "dropout                               0.018282\n",
              "num_layers                                   1\n",
              "num_neurons                                300\n",
              "Topic diversity(not optimized)        0.791304\n",
              "Name: 121, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}